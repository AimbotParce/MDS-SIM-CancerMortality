# Data Cleansing
```{r, echo=FALSE}
library(crayon)
library(car)
library(FactoMineR)
library(chemometrics)
library(corrplot)
library(RColorBrewer)
library(PerformanceAnalytics)
```


```{r}	
df <- read.csv("data/train.csv")
```

## 1. Checking variable types

All variables but the geography should be numeric. Only one variable requires
any change, "binnedinc" is a string variable right now, but we can create a
new inc_capita that will take the midpoint in the bin as its value.

```{r}
# Use regex to remove the [,],( and ) from the rows:
inc.midpoints.text <- gsub("[\\[\\]()]", "", df$binnedinc, perl = T)
# Separate them into two numbers
inc.midpoints.text.sep <- strsplit(inc.midpoints.text, ",")
# Convert them to numbers and apply a mean between them to find the midpoint
df$inc_capita <- sapply(inc.midpoints.text.sep, function(x) mean(as.numeric(x)))
```

## 2. Checking for data duplication

Check if there is any duplicated data
```{r}
duplicated_row_count <- sum(duplicated(df))
if (duplicated_row_count > 0) {
    print(sprintf("There are %d duplicated rows.", duplicated_row_count))
    df <- unique(df)
}
```

## 3. Handling outliers

We will, however, find outliers in the data, then we'll asses them and determine
whether they should or should not be removed.

### Univariate outliers

```{r}
compute_outliers <- function(df) {
    severe <- list()
    severe$outliers <- list()
    severe$limits <- list()

    mild <- list()
    mild$outliers <- list()
    mild$limits <- list()

    for (colname in colnames(Filter(is.numeric, df))) {
        col <- df[[colname]]

        summ <- summary(col)
        q1 <- summ["1st Qu."]
        q3 <- summ["3rd Qu."]
        iqr <- q3 - q1

        severe$limits[[colname]] <- list(top = q3 + 3 * iqr, bot = q1 - 3 * iqr)
        mild$limits[[colname]] <- list(top = q3 + 1.5 * iqr, bot = q1 - 1.5 * iqr)

        severe$outliers[[colname]] <- which(col > severe$limits[[colname]]$top | col < severe$limits[[colname]]$bot)
        mild$outliers[[colname]] <- which(
            (col > mild$limits[[colname]]$top & col < severe$limits[[colname]]$top) |
                (col < mild$limits[[colname]]$bot & col > severe$limits[[colname]]$bot)
        )
    }
    return(list(mild = mild, severe = severe))
}

o <- compute_outliers(df)

for (colname in colnames(Filter(is.numeric, df))) {
    mild_col <- if (length(o$mild$outliers[[colname]]) > 0) yellow else green
    seve_col <- if (length(o$severe$outliers[[colname]]) > 0) red else green
    mild_text <- mild_col(sprintf("%d mild outliers", length(mild_outliers[[colname]])))
    seve_text <- seve_col(sprintf("%d severe outliers", length(severe_outliers[[colname]])))
    cat(sprintf("Column %s has %s and %s\n", colname, mild_text, seve_text))
}
```

Let's visualize it with a multi-box-plot.

```{r}
boxplot_outliers <- function(df, severe_outliers, mild_outliers, severe_limits, mild_limits) {
    numeric_cols <- colnames(Filter(is.numeric, df))
    par(mfrow = c(2, ceiling(length(numeric_cols) / 2)), cex.main = 1.5, cex.axis = 1.5)
    for (colname in numeric_cols) {
        severe_points <- df[[colname]][severe_outliers[[colname]]]
        mild_points <- df[[colname]][mild_outliers[[colname]]]
        title_col <- if (length(severe_points) > 0) "red" else "black"
        boxplot(df[[colname]], main = colname, col.main = title_col, cex = 1.5)
        abline(h = severe_limits[[colname]]$top, col = "red")
        abline(h = severe_limits[[colname]]$bot, col = "red")
        abline(h = mild_limits[[colname]]$top, col = "orange")
        abline(h = mild_limits[[colname]]$top, col = "orange")
        points(x = rep(1, length(severe_points)), y = severe_points, col = "red", cex = 1.5, pch = 19)
        points(x = rep(1, length(mild_points)), y = mild_points, col = "orange", cex = 1.5, pch = 19)
    }
}

png("images/boxplot_outliers.png", width = 2300, height = 1200)
boxplot_outliers(df, o$severe$outliers, o$mild$outliers, o$severe$limits, o$mild$limits)
dev.off()
```

We can see that by the criteria chosen, too many data points must be set to NA.
There's, however, only five variables whose outliers I would consider removing:
We'll be removing outliers from `target_deathrate`, because not only it's the 
target, but also it contains a single severe outlier quite far from the rest of
the data points.

Also, outliers from `incidencerate`, `pctbachdeg25_over` and 
`pctunemployed16_over`, we'll also remove, as they are very few, and are well 
outside their range to be considered for the study.

Finally, one variable raised our suspicions: `medianage`, which has 18 outliers
that are somehow clustered really far from the rest. Also, given that this 
column represents median ages, we know as a fact that 300+ years makes no sense.

```{r}
outlier_remove_columns <- c(
    "target_deathrate", "incidencerate", "pctbachdeg25_over",
    "pctunemployed16_over", "medianage"
)

for (colname in outlier_remove_columns) {
    df[[colname]][o$severe$outliers[[colname]]] <- NA
}
```

We can repeat the previous plot to see how it changed:

```{r}
o <- compute_outliers(df)

png("images/boxplot_outliers_after.png", width = 2300, height = 1200)
boxplot_outliers(df, o$severe$outliers, o$mild$outliers, o$severe$limits, o$mild$limits)
dev.off()
```

### Multivariate outliers

We'll use a 95% quantile for the mahalanobis distance to detect multivariate
outliers. Calling function `Moutlier` directly throws an error. Let's search for
which two columns are causing it.

```{r}
png("images/correlation_chart.png", width = 1080, height = 1080)
chart.Correlation(Filter(is.numeric, df), histogram = TRUE, pch = 19, method = "pearson", use = "na.or.complete")
dev.off()
```

## 4. Handling missing data

Let's see if we have any missing data.

```{r}
for (colname in colnames(df)) {
    na.count <- sum(is.na(df[[colname]]))
    if (na.count > 0) {
        cat(sprintf("%s has %s\n", colname, red(sprintf("%d N/As", sum(is.na(df[colname]))))))
    }
}
```

### pctemployed16_over 

In our current data, there's 82 N/As in pctemployed16_over . Our first intuition
would be for this value to have exactly a -1 correlation with variable
pctunemployed16_over, which somehow has no missing rows. Let's check that.

```{r}
cor(df$pctemployed16_over, df$pctunemployed16_over, use = "complete.obs")
```

Somehow they are not that closely related as we hopped.

### Imputation of the missing data

We couldn't easily impute the missing data from the `pctemployed16_over`, thus,
for now, we won't be imputing any of the missing data. We'll leave that to be
decided afterwards, because depending on what we want to do we might not want to
have non-real data roaming around.
